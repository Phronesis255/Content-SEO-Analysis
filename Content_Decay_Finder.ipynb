{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75183e8",
   "metadata": {},
   "source": [
    "# ðŸ”Ž Content Decay Finder & Refresh Planner\n",
    "\n",
    "Automatically detect pages whose organic traffic from Google Search has **decayed**, quantify lost clicks, and generate a prioritized refresh plan â€” all powered by the Google Search Console API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbcb98",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. **Google Cloud project** with the *Search Console API* enabled.\n",
    "2. **Serviceâ€‘account JSON** added as a user to the Search Console property.\n",
    "3. Python packages: `google-api-python-client`, `pandas`, `numpy`, `plotly`, `tqdm`.\n",
    "\n",
    "> **Tip**: In Colab, you can upload the JSON key via the sidebar **Files â†’ Upload** and set its path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deps (Colab only â€” comment out locally)\n",
    "!pip -q install google-api-python-client tqdm plotly pandas numpy python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9ba71",
   "metadata": {},
   "source": [
    "### ðŸ”§ Configuration\n",
    "Fill in your property URL and path to the serviceâ€‘account JSON. Tweak the analysis window and decay threshold if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER CONFIG ---\n",
    "PROPERTY_URL = 'https://www.example.com'  # Your exact Search Console property URL or 'sc-domain:example.com'\n",
    "SERVICE_ACCOUNT_FILE = '/content/drive/MyDrive/creds/sc_service_account.json'\n",
    "\n",
    "LOOKBACK_DAYS = 540          # Pull ~18 months of data\n",
    "MOVING_AVG_WEEKS = 12         # Trailing window for moving average\n",
    "DECAY_THRESHOLD = 0.20        # 20% drop triggers flag\n",
    "\n",
    "# --------------------\n",
    "END_DATE = date.today()\n",
    "START_DATE = END_DATE - timedelta(days=LOOKBACK_DAYS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35d8c9",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Authenticate & build Search Console service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES\n",
    ")\n",
    "service = build('searchconsole', 'v1', credentials=creds)\n",
    "print('âœ… Authenticated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca38ed",
   "metadata": {},
   "source": [
    "### â¬‡ï¸ Pull daily Search Console data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43558bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_daily_data(property_url: str, start_date: date, end_date: date) -> pd.DataFrame:\n",
    "    \"\"\"Fetch daily clicks & impressions from GSC.\"\"\"\n",
    "    all_rows = []\n",
    "    request = {\n",
    "        'startDate': start_date.isoformat(),\n",
    "        'endDate': end_date.isoformat(),\n",
    "        'dimensions': ['page', 'date'],\n",
    "        'rowLimit': 25000\n",
    "    }\n",
    "    response = service.searchanalytics().query(siteUrl=property_url, body=request).execute()\n",
    "    rows = response.get('rows', [])\n",
    "    for row in rows:\n",
    "        all_rows.append({\n",
    "            'url': row['keys'][0],\n",
    "            'date': row['keys'][1],\n",
    "            'clicks': row['clicks'],\n",
    "            'impressions': row['impressions']\n",
    "        })\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "df_daily = fetch_daily_data(PROPERTY_URL, START_DATE, END_DATE)\n",
    "print(f'Pulled {len(df_daily):,} daily rows for {df_daily.url.nunique():,} pages')\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff0c2a",
   "metadata": {},
   "source": [
    "### ðŸ“Š Aggregate to weekly & compute decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54832610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to weekly clicks\n",
    "df_weekly = (\n",
    "    df_daily\n",
    "    .set_index('date')\n",
    "    .groupby('url')\n",
    "    .resample('W')\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Compute trailing moving average & previous-period average\n",
    "def detect_decay(df_group):\n",
    "    df_group = df_group.sort_values('date')\n",
    "    df_group['ma'] = df_group['clicks'].rolling(window=MOVING_AVG_WEEKS, min_periods=MOVING_AVG_WEEKS).mean()\n",
    "    # Compare last MA vs same length preceding period\n",
    "    if len(df_group) < MOVING_AVG_WEEKS * 2:\n",
    "        return None  # not enough data\n",
    "    recent_avg = df_group['ma'].iloc[-1]\n",
    "    prior_avg = df_group['ma'].iloc[-MOVING_AVG_WEEKS-1]\n",
    "    decay_pct = (prior_avg - recent_avg) / prior_avg if prior_avg > 0 else 0\n",
    "    lost_clicks = (prior_avg - recent_avg) * MOVING_AVG_WEEKS\n",
    "    return pd.Series({'recent_avg': recent_avg, 'prior_avg': prior_avg, 'decay_pct': decay_pct, 'lost_clicks': lost_clicks})\n",
    "\n",
    "decay_records = []\n",
    "for url, grp in df_weekly.groupby('url'):\n",
    "    rec = detect_decay(grp)\n",
    "    if rec is not None:\n",
    "        rec['url'] = url\n",
    "        decay_records.append(rec)\n",
    "\n",
    "df_decay = pd.DataFrame(decay_records)\n",
    "df_decay = df_decay[df_decay['decay_pct'] >= DECAY_THRESHOLD]\n",
    "df_decay = df_decay.sort_values('lost_clicks', ascending=False).reset_index(drop=True)\n",
    "print(f'Pages flagged: {len(df_decay):,}')\n",
    "df_decay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043511a5",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Visualise lost clicks by URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_decay.head(50),\n",
    "    x='lost_clicks',\n",
    "    y='url',\n",
    "    orientation='h',\n",
    "    title='Top pages by lost clicks',\n",
    "    labels={'lost_clicks': 'Lost clicks (last period vs. previous)', 'url': ''}\n",
    ")\n",
    "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77be25",
   "metadata": {},
   "source": [
    "### ðŸ’¾ Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = '/content/decay_refresh_plan.csv'\n",
    "df_decay.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f'âœ… Saved to {OUTPUT_CSV}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
